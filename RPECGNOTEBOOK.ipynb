{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement des données MIT-BIH...\n",
      "Téléchargement de l'enregistrement 100...\n",
      "Signal shape: (650000, 2)\n",
      "Nombre d'annotations: 2274\n",
      "Segments extraits de 100: 2271\n",
      "Téléchargement de l'enregistrement 101...\n",
      "Signal shape: (650000, 2)\n",
      "Nombre d'annotations: 1874\n",
      "Segments extraits de 101: 4143\n",
      "Téléchargement de l'enregistrement 102...\n",
      "Signal shape: (650000, 2)\n",
      "Nombre d'annotations: 2192\n",
      "Segments extraits de 102: 6334\n",
      "Téléchargement de l'enregistrement 103...\n",
      "Signal shape: (650000, 2)\n",
      "Nombre d'annotations: 2091\n",
      "Segments extraits de 103: 8424\n",
      "Téléchargement de l'enregistrement 104...\n",
      "Signal shape: (650000, 2)\n",
      "Nombre d'annotations: 2311\n",
      "Segments extraits de 104: 10733\n",
      "Données finales: 10733 segments\n",
      "Forme des données: (10733, 200)\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Télécharger et traiter les données MIT-BIH\n",
    "def download_mitbih_data():\n",
    "    records = ['100', '101', '102', '103', '104']\n",
    "    all_signals = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for record in records:\n",
    "        try:\n",
    "            print(f\"Téléchargement de l'enregistrement {record}...\")\n",
    "            \n",
    "            # Télécharger les données\n",
    "            signal, fields = wfdb.rdsamp(record, pn_dir='mitdb')\n",
    "            annotation = wfdb.rdann(record, 'atr', pn_dir='mitdb')\n",
    "            \n",
    "            print(f\"Signal shape: {signal.shape}\")\n",
    "            print(f\"Nombre d'annotations: {len(annotation.sample)}\")\n",
    "            \n",
    "            # Utiliser le premier canal\n",
    "            ecg_signal = signal[:, 0]\n",
    "            \n",
    "            # Créer des segments autour de chaque battement détecté\n",
    "            for i, (sample_idx, symbol) in enumerate(zip(annotation.sample, annotation.symbol)):\n",
    "                # Prendre une fenêtre de 200 points autour de chaque battement\n",
    "                start_idx = max(0, sample_idx - 100)\n",
    "                end_idx = min(len(ecg_signal), sample_idx + 100)\n",
    "                \n",
    "                if end_idx - start_idx >= 200:\n",
    "                    segment = ecg_signal[start_idx:start_idx + 200]\n",
    "                    \n",
    "                    # Mapper les annotations vers 5 classes principales\n",
    "                    if symbol in ['N', 'L', 'R', 'e', 'j', '.']:\n",
    "                        label = 'N'  # Normal\n",
    "                    elif symbol in ['A', 'a', 'J', 'S']:\n",
    "                        label = 'S'  # Supraventricular\n",
    "                    elif symbol in ['V', 'E']:\n",
    "                        label = 'V'  # Ventricular\n",
    "                    elif symbol in ['F']:\n",
    "                        label = 'F'  # Fusion\n",
    "                    else:\n",
    "                        label = 'Q'  # Unclassifiable\n",
    "                    \n",
    "                    all_signals.append(segment)\n",
    "                    all_labels.append(label)\n",
    "            \n",
    "            print(f\"Segments extraits de {record}: {len([l for l in all_labels if l in ['N', 'S', 'V', 'F', 'Q']])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {record}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_signals) == 0:\n",
    "        print(\"Aucune donnée téléchargée. Création de données synthétiques pour test...\")\n",
    "        # Créer des données factices pour le test\n",
    "        np.random.seed(42)\n",
    "        for i in range(1000):\n",
    "            # Signal ECG synthétique\n",
    "            t = np.linspace(0, 1, 200)\n",
    "            signal = np.sin(2*np.pi*t) + 0.5*np.sin(4*np.pi*t) + np.random.normal(0, 0.1, 200)\n",
    "            all_signals.append(signal)\n",
    "            \n",
    "            # Labels aléatoires\n",
    "            labels = ['N', 'S', 'V', 'F', 'Q']\n",
    "            all_labels.append(np.random.choice(labels))\n",
    "        \n",
    "        print(\"1000 échantillons synthétiques créés pour le test\")\n",
    "    \n",
    "    return np.array(all_signals), np.array(all_labels)\n",
    "\n",
    "print(\"Téléchargement des données MIT-BIH...\")\n",
    "X, y = download_mitbih_data()\n",
    "print(f\"Données finales: {len(X)} segments\")\n",
    "print(f\"Forme des données: {X.shape}\")\n",
    "\n",
    "# Vérification que les données ne sont pas vides\n",
    "if len(X) == 0:\n",
    "    raise ValueError(\"Aucune donnée n'a été téléchargée!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdcd7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes disponibles: ['N' 'Q' 'S' 'V']\n",
      "Distribution des classes:\n",
      "N: 6440\n",
      "Q: 4248\n",
      "S: 38\n",
      "V: 7\n",
      "Avant SMOTE: 8586 échantillons\n",
      "Après SMOTE: 20608 échantillons\n"
     ]
    }
   ],
   "source": [
    "# Normalisation des signaux\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Classes disponibles:\", label_encoder.classes_)\n",
    "print(\"Distribution des classes:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"{u}: {c}\")\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Application de SMOTE pour équilibrer les classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Avant SMOTE: {len(X_train)} échantillons\")\n",
    "print(f\"Après SMOTE: {len(X_train_balanced)} échantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8ed2362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement Decision Tree...\n",
      "Entraînement Random Forest...\n",
      "Entraînement XGBoost...\n",
      "Entraînement CNN...\n",
      "Shape des données CNN: (20608, 200, 1)\n",
      "Taille d'entrée pour CNN: 200\n",
      "Début de l'entraînement CNN...\n",
      "Epoch 1/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.6299 - loss: 0.7730 - val_accuracy: 1.0000 - val_loss: 0.0286\n",
      "Epoch 2/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9045 - loss: 0.2612 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 3/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9422 - loss: 0.1641 - val_accuracy: 1.0000 - val_loss: 0.0053\n",
      "Epoch 4/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.9537 - loss: 0.1307 - val_accuracy: 1.0000 - val_loss: 7.4543e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9629 - loss: 0.1088 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 6/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9645 - loss: 0.1025 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
      "Epoch 7/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9689 - loss: 0.0904 - val_accuracy: 1.0000 - val_loss: 2.9113e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.9708 - loss: 0.0837 - val_accuracy: 1.0000 - val_loss: 3.8674e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9742 - loss: 0.0753 - val_accuracy: 1.0000 - val_loss: 3.5769e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9739 - loss: 0.0776 - val_accuracy: 1.0000 - val_loss: 2.3513e-04\n",
      "Prédictions CNN...\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire pour stocker les résultats\n",
    "results = {}\n",
    "\n",
    "# 1. Decision Tree\n",
    "print(\"Entraînement Decision Tree...\")\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "results['Decision Tree'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_dt) * 100,\n",
    "    'Precision': precision_score(y_test, y_pred_dt, average='weighted') * 100,\n",
    "    'Recall': recall_score(y_test, y_pred_dt, average='weighted') * 100,\n",
    "    'F1-Score': f1_score(y_test, y_pred_dt, average='weighted') * 100\n",
    "}\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"Entraînement Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_rf) * 100,\n",
    "    'Precision': precision_score(y_test, y_pred_rf, average='weighted') * 100,\n",
    "    'Recall': recall_score(y_test, y_pred_rf, average='weighted') * 100,\n",
    "    'F1-Score': f1_score(y_test, y_pred_rf, average='weighted') * 100\n",
    "}\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"Entraînement XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "results['XGBoost'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_xgb) * 100,\n",
    "    'Precision': precision_score(y_test, y_pred_xgb, average='weighted') * 100,\n",
    "    'Recall': recall_score(y_test, y_pred_xgb, average='weighted') * 100,\n",
    "    'F1-Score': f1_score(y_test, y_pred_xgb, average='weighted') * 100\n",
    "}\n",
    "\n",
    "# 4. CNN\n",
    "print(\"Entraînement CNN...\")\n",
    "# Reshape pour CNN (samples, timesteps, features)\n",
    "X_train_cnn = X_train_balanced.reshape(X_train_balanced.shape[0], X_train_balanced.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "print(f\"Shape des données CNN: {X_train_cnn.shape}\")\n",
    "\n",
    "# Convertir les labels en categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_categorical = to_categorical(y_train_balanced)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Obtenir la taille d'entrée dynamiquement\n",
    "input_shape = X_train_cnn.shape[1]\n",
    "print(f\"Taille d'entrée pour CNN: {input_shape}\")\n",
    "\n",
    "# Modèle CNN adapté à la taille des données\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(input_shape, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement (réduit pour le test)\n",
    "print(\"Début de l'entraînement CNN...\")\n",
    "history = model.fit(X_train_cnn, y_train_categorical, \n",
    "                   epochs=10, batch_size=32, \n",
    "                   validation_split=0.2, verbose=1)\n",
    "\n",
    "# Prédictions CNN\n",
    "print(\"Prédictions CNN...\")\n",
    "y_pred_cnn_prob = model.predict(X_test_cnn, verbose=0)\n",
    "y_pred_cnn = np.argmax(y_pred_cnn_prob, axis=1)\n",
    "\n",
    "results['CNN'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_cnn) * 100,\n",
    "    'Precision': precision_score(y_test, y_pred_cnn, average='weighted') * 100,\n",
    "    'Recall': recall_score(y_test, y_pred_cnn, average='weighted') * 100,\n",
    "    'F1-Score': f1_score(y_test, y_pred_cnn, average='weighted') * 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c164db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RÉSULTATS (Tableau 2 de l'article) ===\n",
      "               Accuracy  Precision  Recall  F1-Score\n",
      "Decision Tree     98.46      98.59   98.46     98.52\n",
      "Random Forest     99.49      99.44   99.49     99.38\n",
      "XGBoost           99.30      99.22   99.30     99.26\n",
      "CNN               98.98      99.37   98.98     99.12\n",
      "\n",
      "=== RÉSULTATS ATTENDUS (Article) ===\n",
      "               Accuracy  Precision  Recall  F1-Score\n",
      "CNN                  99         98      98        98\n",
      "Decision Tree        78         73      76        93\n",
      "Random Forest        98         97      98        98\n",
      "XGBoost              97         97      97        97\n",
      "\n",
      "=== DIFFÉRENCES ===\n",
      "\n",
      "Decision Tree:\n",
      "  Accuracy: 98.5% (attendu: 78%, diff: +20.5%)\n",
      "  Precision: 98.6% (attendu: 73%, diff: +25.6%)\n",
      "  Recall: 98.5% (attendu: 76%, diff: +22.5%)\n",
      "  F1-Score: 98.5% (attendu: 93%, diff: +5.5%)\n",
      "\n",
      "Random Forest:\n",
      "  Accuracy: 99.5% (attendu: 98%, diff: +1.5%)\n",
      "  Precision: 99.4% (attendu: 97%, diff: +2.4%)\n",
      "  Recall: 99.5% (attendu: 98%, diff: +1.5%)\n",
      "  F1-Score: 99.4% (attendu: 98%, diff: +1.4%)\n",
      "\n",
      "XGBoost:\n",
      "  Accuracy: 99.3% (attendu: 97%, diff: +2.3%)\n",
      "  Precision: 99.2% (attendu: 97%, diff: +2.2%)\n",
      "  Recall: 99.3% (attendu: 97%, diff: +2.3%)\n",
      "  F1-Score: 99.3% (attendu: 97%, diff: +2.3%)\n",
      "\n",
      "CNN:\n",
      "  Accuracy: 99.0% (attendu: 99%, diff: -0.0%)\n",
      "  Precision: 99.4% (attendu: 98%, diff: +1.4%)\n",
      "  Recall: 99.0% (attendu: 98%, diff: +1.0%)\n",
      "  F1-Score: 99.1% (attendu: 98%, diff: +1.1%)\n"
     ]
    }
   ],
   "source": [
    "# Création du tableau de résultats\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results = df_results.round(2)\n",
    "\n",
    "print(\"\\n=== RÉSULTATS (Tableau 2 de l'article) ===\")\n",
    "print(df_results)\n",
    "\n",
    "# Comparaison avec les résultats attendus de l'article\n",
    "expected_results = {\n",
    "    'CNN': {'Accuracy': 99, 'Precision': 98, 'Recall': 98, 'F1-Score': 98},\n",
    "    'Decision Tree': {'Accuracy': 78, 'Precision': 73, 'Recall': 76, 'F1-Score': 93},\n",
    "    'Random Forest': {'Accuracy': 98, 'Precision': 97, 'Recall': 98, 'F1-Score': 98},\n",
    "    'XGBoost': {'Accuracy': 97, 'Precision': 97, 'Recall': 97, 'F1-Score': 97}\n",
    "}\n",
    "\n",
    "print(\"\\n=== RÉSULTATS ATTENDUS (Article) ===\")\n",
    "df_expected = pd.DataFrame(expected_results).T\n",
    "print(df_expected)\n",
    "\n",
    "print(\"\\n=== DIFFÉRENCES ===\")\n",
    "for model in results.keys():\n",
    "    if model in expected_results:\n",
    "        print(f\"\\n{model}:\")\n",
    "        for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "            obtained = results[model][metric]\n",
    "            expected = expected_results[model][metric]\n",
    "            diff = obtained - expected\n",
    "            print(f\"  {metric}: {obtained:.1f}% (attendu: {expected}%, diff: {diff:+.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
